{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnltk.download('punkt')\\nnltk.download('punkt_tab')\\nnltk.download('stopwords')\\nnltk.download('wordnet')\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.join(current_dir, '..', 'data')\n",
    "file_path = os.path.join(data_dir, 'youtoxic_english_1000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              CommentId      VideoId  \\\n",
      "0  Ugg2KwwX0V8-aXgCoAEC  04kJtp6pVXI   \n",
      "1  Ugg2s5AzSPioEXgCoAEC  04kJtp6pVXI   \n",
      "2  Ugg3dWTOxryFfHgCoAEC  04kJtp6pVXI   \n",
      "3  Ugg7Gd006w1MPngCoAEC  04kJtp6pVXI   \n",
      "4  Ugg8FfTbbNF8IngCoAEC  04kJtp6pVXI   \n",
      "\n",
      "                                                Text  IsToxic  IsAbusive  \\\n",
      "0  people would take step back make case wasnt an...    False      False   \n",
      "1  law enforcement trained shoot apprehend traine...     True       True   \n",
      "2  dont reckon black life matter banner held whit...     True       True   \n",
      "3  large number people like police officer called...    False      False   \n",
      "4  arab dude absolutely right shot 6 extra time s...    False      False   \n",
      "\n",
      "   IsThreat  IsProvocative  IsObscene  IsHatespeech  IsRacist  IsNationalist  \\\n",
      "0     False          False      False         False     False          False   \n",
      "1     False          False      False         False     False          False   \n",
      "2     False          False       True         False     False          False   \n",
      "3     False          False      False         False     False          False   \n",
      "4     False          False      False         False     False          False   \n",
      "\n",
      "   IsSexist  IsHomophobic  IsReligiousHate  IsRadicalism  \n",
      "0     False         False            False         False  \n",
      "1     False         False            False         False  \n",
      "2     False         False            False         False  \n",
      "3     False         False            False         False  \n",
      "4     False         False            False         False  \n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['Text'] = df['Text'].apply(preprocess_text)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                CommentId      VideoId  \\\n",
      "0    Ugg2KwwX0V8-aXgCoAEC  04kJtp6pVXI   \n",
      "1    Ugg2s5AzSPioEXgCoAEC  04kJtp6pVXI   \n",
      "2    Ugg3dWTOxryFfHgCoAEC  04kJtp6pVXI   \n",
      "3    Ugg7Gd006w1MPngCoAEC  04kJtp6pVXI   \n",
      "4    Ugg8FfTbbNF8IngCoAEC  04kJtp6pVXI   \n",
      "..                    ...          ...   \n",
      "995  Ugi5ADt10EdDz3gCoAEC  XRuCW80L9mA   \n",
      "996  Ugifh2DMhBbDkHgCoAEC  XRuCW80L9mA   \n",
      "997  Ugj_plbGBjjzYXgCoAEC  XRuCW80L9mA   \n",
      "998  Ugj0bah1De8xy3gCoAEC  XRuCW80L9mA   \n",
      "999  UgjBJKQSoQMQ6ngCoAEC  XRuCW80L9mA   \n",
      "\n",
      "                                                  Text  IsHateSpeech  \n",
      "0    people would take step back make case wasnt an...             0  \n",
      "1    law enforcement trained shoot apprehend traine...             1  \n",
      "2    dont reckon black life matter banner held whit...             1  \n",
      "3    large number people like police officer called...             0  \n",
      "4    arab dude absolutely right shot 6 extra time s...             0  \n",
      "..                                                 ...           ...  \n",
      "995                     remember sent national defence             0  \n",
      "996  stats dont represent problem race baiting atti...             1  \n",
      "997                 quote mother wow hit hard accurate             0  \n",
      "998                                       video racist             0  \n",
      "999                         god narrator annoying lisp             0  \n",
      "\n",
      "[1000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "hate_speech_columns = [col for col in df.columns if col.startswith('Is')]\n",
    "df['IsHateSpeech'] = df[hate_speech_columns].any(axis=1).astype(int)\n",
    "df.drop(columns=hate_speech_columns, inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X = vectorizer.fit_transform(df['Text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())  # Convert the sparse matrix to DataFrame\n",
    "X_df['IsHateSpeech'] = df['IsHateSpeech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_df.to_csv(os.path.join(data_dir, 'preprocessed_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\iryna\\\\Desktop\\\\NLP_Youtube_9\\\\notebooks\\\\..\\\\data\\\\tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_filename = os.path.join(data_dir, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Save the vectorizer to the specified directory\n",
    "joblib.dump(vectorizer, vectorizer_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
