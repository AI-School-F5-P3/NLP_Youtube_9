{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnltk.download('punkt')\\nnltk.download('punkt_tab')\\nnltk.download('stopwords')\\nnltk.download('wordnet')\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.join(current_dir, '..', 'data')\n",
    "file_path = os.path.join(data_dir, 'youtoxic_english_1000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              CommentId      VideoId  \\\n",
      "0  Ugg2KwwX0V8-aXgCoAEC  04kJtp6pVXI   \n",
      "1  Ugg2s5AzSPioEXgCoAEC  04kJtp6pVXI   \n",
      "2  Ugg3dWTOxryFfHgCoAEC  04kJtp6pVXI   \n",
      "3  Ugg7Gd006w1MPngCoAEC  04kJtp6pVXI   \n",
      "4  Ugg8FfTbbNF8IngCoAEC  04kJtp6pVXI   \n",
      "\n",
      "                                                Text  IsToxic  IsAbusive  \\\n",
      "0  people would take step back make case wasnt an...    False      False   \n",
      "1  law enforcement trained shoot apprehend traine...     True       True   \n",
      "2  dont reckon black life matter banner held whit...     True       True   \n",
      "3  large number people like police officer called...    False      False   \n",
      "4  arab dude absolutely right shot 6 extra time s...    False      False   \n",
      "\n",
      "   IsThreat  IsProvocative  IsObscene  IsHatespeech  IsRacist  IsNationalist  \\\n",
      "0     False          False      False         False     False          False   \n",
      "1     False          False      False         False     False          False   \n",
      "2     False          False       True         False     False          False   \n",
      "3     False          False      False         False     False          False   \n",
      "4     False          False      False         False     False          False   \n",
      "\n",
      "   IsSexist  IsHomophobic  IsReligiousHate  IsRadicalism  \n",
      "0     False         False            False         False  \n",
      "1     False         False            False         False  \n",
      "2     False         False            False         False  \n",
      "3     False         False            False         False  \n",
      "4     False         False            False         False  \n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['Text'] = df['Text'].apply(preprocess_text)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = [col for col in df.columns if col.startswith('Is')]\n",
    "vectorizers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target column: IsToxic\n",
      "Train and test data for IsToxic processed!\n",
      "Processing target column: IsAbusive\n",
      "Train and test data for IsAbusive processed!\n",
      "Processing target column: IsThreat\n",
      "Train and test data for IsThreat processed!\n",
      "Processing target column: IsProvocative\n",
      "Train and test data for IsProvocative processed!\n",
      "Processing target column: IsObscene\n",
      "Train and test data for IsObscene processed!\n",
      "Processing target column: IsHatespeech\n",
      "Train and test data for IsHatespeech processed!\n",
      "Processing target column: IsRacist\n",
      "Train and test data for IsRacist processed!\n",
      "Processing target column: IsNationalist\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors <= n_samples_fit, but n_neighbors = 6, n_samples_fit = 4, n_samples = 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Apply ADASYN to balance the training data (oversample minority class)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m adasyn \u001b[38;5;241m=\u001b[39m ADASYN(sampling_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminority\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m X_train_resampled, y_train_resampled \u001b[38;5;241m=\u001b[39m \u001b[43madasyn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Convert the resampled data to a DataFrame for train and test sets\u001b[39;00m\n\u001b[0;32m     27\u001b[0m train_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_train_resampled\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mtfidf\u001b[38;5;241m.\u001b[39mget_feature_names_out())  \u001b[38;5;66;03m# Convert sparse matrix to array\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\imblearn\\base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\imblearn\\base.py:112\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    106\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[0;32m    110\u001b[0m )\n\u001b[1;32m--> 112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    115\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    116\u001b[0m )\n\u001b[0;32m    118\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m arrays_transformer\u001b[38;5;241m.\u001b[39mtransform(output[\u001b[38;5;241m0\u001b[39m], y_)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\imblearn\\over_sampling\\_adasyn.py:202\u001b[0m, in \u001b[0;36mADASYN._fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# the nearest neighbors need to be fitted only on the current class\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;66;03m# to find the class NN to generate new samples\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_\u001b[38;5;241m.\u001b[39mfit(X_class)\n\u001b[1;32m--> 202\u001b[0m nns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    204\u001b[0m enumerated_class_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(target_class_indices))\n\u001b[0;32m    205\u001b[0m rows \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(enumerated_class_indices, n_samples_generate)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\neighbors\\_base.py:834\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    832\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m         inequality_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_neighbors <= n_samples_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 834\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    835\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minequality_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    836\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_neighbors = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_neighbors\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, n_samples_fit = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples_fit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    837\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# include n_samples for common tests\u001b[39;00m\n\u001b[0;32m    838\u001b[0m     )\n\u001b[0;32m    840\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m    841\u001b[0m chunked_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected n_neighbors <= n_samples_fit, but n_neighbors = 6, n_samples_fit = 4, n_samples = 4"
     ]
    }
   ],
   "source": [
    "all_train_data = pd.DataFrame()\n",
    "all_test_data = pd.DataFrame()\n",
    "\n",
    "for target_column in target_columns:\n",
    "    print(f\"Processing target column: {target_column}\")\n",
    "\n",
    "    X = df['Text']\n",
    "    y = df[target_column]  \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "    # Fit and transform the training data, transform the test data\n",
    "    X_train_tfidf = tfidf.fit_transform(X_train) \n",
    "    X_test_tfidf = tfidf.transform(X_test)  \n",
    "\n",
    "    # Save the vectorizer for later use\n",
    "    vectorizers[target_column] = tfidf\n",
    "\n",
    "    # Apply ADASYN to balance the training data (oversample minority class)\n",
    "    adasyn = ADASYN(sampling_strategy='minority', n_neighbors=3, random_state=42)\n",
    "    X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "    # Convert the resampled data to a DataFrame for train and test sets\n",
    "    train_data = pd.DataFrame(X_train_resampled.toarray(), columns=tfidf.get_feature_names_out())  # Convert sparse matrix to array\n",
    "    train_data[target_column] = y_train_resampled\n",
    "\n",
    "    test_data = pd.DataFrame(X_test_tfidf.toarray(), columns=tfidf.get_feature_names_out())  # Convert sparse matrix to array\n",
    "    test_data[target_column] = y_test\n",
    "\n",
    "    # Append the train and test data to the overall DataFrames\n",
    "    all_train_data = pd.concat([all_train_data, train_data], axis=1)\n",
    "    all_test_data = pd.concat([all_test_data, test_data], axis=1)\n",
    "\n",
    "    print(f\"Train and test data for {target_column} processed!\")\n",
    "\n",
    "# Save the concatenated train and test data for all target columns\n",
    "train_file_path = os.path.join(data_dir, 'all_train_data.csv')\n",
    "test_file_path = os.path.join(data_dir, 'all_test_data.csv')\n",
    "\n",
    "# Save as CSV\n",
    "all_train_data.to_csv(train_file_path, index=False)\n",
    "all_test_data.to_csv(test_file_path, index=False)\n",
    "\n",
    "# Save the vectorizers for all target columns\n",
    "vectorizer_file_path = os.path.join(data_dir, 'all_tfidf_vectorizers.joblib')\n",
    "joblib.dump(vectorizers, vectorizer_file_path)\n",
    "\n",
    "print(f\"All target columns have been processed, vectorized, oversampled, and saved!\")\n",
    "print(f\"Train data saved to: {train_file_path}\")\n",
    "print(f\"Test data saved to: {test_file_path}\")\n",
    "print(f\"All TF-IDF vectorizers have been saved to: {vectorizer_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
