{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnltk.download('punkt')\\nnltk.download('punkt_tab')\\nnltk.download('stopwords')\\nnltk.download('wordnet')\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.join(current_dir, '..', 'data')\n",
    "file_path = os.path.join(data_dir, 'youtoxic_english_1000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              CommentId      VideoId  \\\n",
      "0  Ugg2KwwX0V8-aXgCoAEC  04kJtp6pVXI   \n",
      "1  Ugg2s5AzSPioEXgCoAEC  04kJtp6pVXI   \n",
      "2  Ugg3dWTOxryFfHgCoAEC  04kJtp6pVXI   \n",
      "3  Ugg7Gd006w1MPngCoAEC  04kJtp6pVXI   \n",
      "4  Ugg8FfTbbNF8IngCoAEC  04kJtp6pVXI   \n",
      "\n",
      "                                                Text  IsToxic  IsAbusive  \\\n",
      "0  people would take step back make case wasnt an...    False      False   \n",
      "1  law enforcement trained shoot apprehend traine...     True       True   \n",
      "2  dont reckon black life matter banner held whit...     True       True   \n",
      "3  large number people like police officer called...    False      False   \n",
      "4  arab dude absolutely right shot 6 extra time s...    False      False   \n",
      "\n",
      "   IsThreat  IsProvocative  IsObscene  IsHatespeech  IsRacist  IsNationalist  \\\n",
      "0     False          False      False         False     False          False   \n",
      "1     False          False      False         False     False          False   \n",
      "2     False          False       True         False     False          False   \n",
      "3     False          False      False         False     False          False   \n",
      "4     False          False      False         False     False          False   \n",
      "\n",
      "   IsSexist  IsHomophobic  IsReligiousHate  IsRadicalism  \n",
      "0     False         False            False         False  \n",
      "1     False         False            False         False  \n",
      "2     False         False            False         False  \n",
      "3     False         False            False         False  \n",
      "4     False         False            False         False  \n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['Text'] = df['Text'].apply(preprocess_text)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = [col for col in df.columns if col.startswith('Is')]\n",
    "vectorizers = {}\n",
    "for target_column in target_columns:\n",
    "    if df[target_column].nunique() == 1:  # Only one unique value (False)\n",
    "        print(f\"Removing column {target_column} due to lack of variability.\")\n",
    "        df.drop(target_column, axis=1, inplace=True)\n",
    "        df.drop('IsSexist', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target column: IsToxic\n",
      "Class distribution for IsToxic:\n",
      "IsToxic\n",
      "False    53.8\n",
      "True     46.2\n",
      "Name: proportion, dtype: float64\n",
      "Train and test data for IsToxic processed!\n",
      "Processing target column: IsAbusive\n",
      "Class distribution for IsAbusive:\n",
      "IsAbusive\n",
      "False    64.7\n",
      "True     35.3\n",
      "Name: proportion, dtype: float64\n",
      "Train and test data for IsAbusive processed!\n",
      "Processing target column: IsThreat\n",
      "Class distribution for IsThreat:\n",
      "IsThreat\n",
      "False    97.9\n",
      "True      2.1\n",
      "Name: proportion, dtype: float64\n",
      "Skipping resampling for IsThreat due to extreme imbalance.\n",
      "Train and test data for IsThreat processed!\n",
      "Processing target column: IsProvocative\n",
      "Class distribution for IsProvocative:\n",
      "IsProvocative\n",
      "False    83.9\n",
      "True     16.1\n",
      "Name: proportion, dtype: float64\n",
      "Train and test data for IsProvocative processed!\n",
      "Processing target column: IsObscene\n",
      "Class distribution for IsObscene:\n",
      "IsObscene\n",
      "False    90.0\n",
      "True     10.0\n",
      "Name: proportion, dtype: float64\n",
      "Train and test data for IsObscene processed!\n",
      "Processing target column: IsHatespeech\n",
      "Class distribution for IsHatespeech:\n",
      "IsHatespeech\n",
      "False    86.2\n",
      "True     13.8\n",
      "Name: proportion, dtype: float64\n",
      "Train and test data for IsHatespeech processed!\n",
      "Processing target column: IsRacist\n",
      "Class distribution for IsRacist:\n",
      "IsRacist\n",
      "False    87.5\n",
      "True     12.5\n",
      "Name: proportion, dtype: float64\n",
      "Train and test data for IsRacist processed!\n",
      "Processing target column: IsNationalist\n",
      "Class distribution for IsNationalist:\n",
      "IsNationalist\n",
      "False    99.2\n",
      "True      0.8\n",
      "Name: proportion, dtype: float64\n",
      "Skipping resampling for IsNationalist due to extreme imbalance.\n",
      "Train and test data for IsNationalist processed!\n",
      "Processing target column: IsReligiousHate\n",
      "Class distribution for IsReligiousHate:\n",
      "IsReligiousHate\n",
      "False    98.8\n",
      "True      1.2\n",
      "Name: proportion, dtype: float64\n",
      "Skipping resampling for IsReligiousHate due to extreme imbalance.\n",
      "Train and test data for IsReligiousHate processed!\n",
      "All target columns have been processed, vectorized, oversampled, and saved!\n",
      "Train data saved to: c:\\Users\\iryna\\Desktop\\NLP_Youtube_9\\notebooks\\..\\data\\dropped_train_data.csv\n",
      "Test data saved to: c:\\Users\\iryna\\Desktop\\NLP_Youtube_9\\notebooks\\..\\data\\dropped_test_data.csv\n",
      "All TF-IDF vectorizers have been saved to: c:\\Users\\iryna\\Desktop\\NLP_Youtube_9\\notebooks\\..\\data\\all_tfidf_vectorizers.joblib\n"
     ]
    }
   ],
   "source": [
    "for target_column in target_columns:\n",
    "    print(f\"Processing target column: {target_column}\")\n",
    "\n",
    "    X = df['Text']\n",
    "    y = df[target_column]\n",
    "\n",
    "    class_counts = y.value_counts(normalize=True) * 100\n",
    "    print(f\"Class distribution for {target_column}:\")\n",
    "    print(class_counts)\n",
    "\n",
    "    if class_counts.max() > 90:\n",
    "        print(f\"Skipping resampling for {target_column} due to extreme imbalance.\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "        tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "        X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "        X_test_tfidf = tfidf.transform(X_test)\n",
    "        train_data = pd.DataFrame(X_train_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n",
    "        train_data[target_column] = y_train\n",
    "        test_data = pd.DataFrame(X_test_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n",
    "        test_data[target_column] = y_test\n",
    "\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "        tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "        X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "        X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "        smote_tomek = SMOTETomek(sampling_strategy='minority', random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "        train_data = pd.DataFrame(X_train_resampled.toarray(), columns=tfidf.get_feature_names_out())\n",
    "        train_data[target_column] = y_train_resampled\n",
    "\n",
    "        test_data = pd.DataFrame(X_test_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n",
    "        test_data[target_column] = y_test\n",
    "\n",
    "    vectorizers[target_column] = tfidf\n",
    "\n",
    "    all_train_data = pd.concat([all_train_data, train_data], axis=1)\n",
    "    all_test_data = pd.concat([all_test_data, test_data], axis=1)\n",
    "\n",
    "    print(f\"Train and test data for {target_column} processed!\")\n",
    "\n",
    "train_file_path = os.path.join(data_dir, 'dropped_train_data.csv')\n",
    "test_file_path = os.path.join(data_dir, 'dropped_test_data.csv')\n",
    "\n",
    "all_train_data.to_csv(train_file_path, index=False)\n",
    "all_test_data.to_csv(test_file_path, index=False)\n",
    "\n",
    "vectorizer_file_path = os.path.join(data_dir, 'all_tfidf_vectorizers.joblib')\n",
    "joblib.dump(vectorizers, vectorizer_file_path)\n",
    "\n",
    "print(f\"All target columns have been processed, vectorized, oversampled, and saved!\")\n",
    "print(f\"Train data saved to: {train_file_path}\")\n",
    "print(f\"Test data saved to: {test_file_path}\")\n",
    "print(f\"All TF-IDF vectorizers have been saved to: {vectorizer_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
